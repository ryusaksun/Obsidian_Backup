## 大型语言模型中编码器与解码器架构深度研究报告：原理、演进与未来前沿

### 1. 引言：神经序列建模的范式转移与架构二分

自2017年Google Brain团队发表《Attention Is All You Need》以来，Transformer架构已彻底重塑了自然语言处理（NLP）乃至整个人工智能领域的格局。这一架构的核心创新在于摒弃了循环神经网络（RNN）和长短期记忆网络（LSTM）的顺序处理归纳偏置，转而采用全注意力的并行计算机制 1。然而，Transformer的原始设计并非单一整体，而是由两个功能截然不同但紧密耦合的组件构成：编码器（Encoder）与解码器（Decoder）。这两个组件分别代表了序列建模中的两大核心任务——理解与生成。

本报告旨在对LLM中编码器与解码器的原理、应用及最新发展现状进行详尽的调查与分析。我们将深入探讨从早期的BERT（Encoder-only）和GPT（Decoder-only）的分野，到T5（Encoder-Decoder）的统一尝试，再到如今DeepSeek-V3、Gemini 1.5 Pro等前沿模型中对混合专家（MoE）、多头潜在注意力（MLA）以及线性状态空间模型（SSM）的创新性融合。报告将特别关注2024年至2026年初的架构突破，包括测试时训练（Test-Time Training, TTT）和Titans等动态记忆架构，揭示计算效率与模型智能之间的深层权衡关系。

---

### 2. 核心原理剖析：Transformer架构的解构与重组

要理解现代LLM的演变，首先必须对Transformer的基础组件进行数学与逻辑层面的深度解构。编码器和解码器虽然共享由于多头自注意力（Multi-Head Self-Attention）和前馈神经网络（FFN）构成的基本模块，但其内部的信息流向、注意力掩码机制以及训练目标存在本质差异。

#### 2.1 编码器（Encoder）：双向语境的深度表征

编码器的核心使命是将离散的输入符号序列（Token Sequence）转化为连续的、包含丰富语义信息的上下文向量（Contextual Embeddings）。在原始Transformer论文中，编码器由6层相同的模块堆叠而成，每一层包含两个子层：多头自注意力机制和位置前馈网络 1。

##### 2.1.1 全局可见性与双向注意力

编码器的最显著特征是其双向性（Bidirectionality）。在自注意力层的计算过程中，序列中的每一个Token都可以看见并关注序列中的其他所有Token，无论其位置是在当前Token之前还是之后 1。

数学上，对于输入嵌入矩阵 $X$，注意力计算如下：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中，$Q$（查询）、$K$（键）、$V$（值）均由输入 $X$ 线性变换而来。在编码器中，注意力分数矩阵 $QK^T$ 不受任何掩码限制（除了填充符掩码），这意味着第 $i$ 个位置的Token在计算其表征时，聚合了来自 $1$ 到 $n$ 所有位置的信息。这种全排列的连接方式使得编码器能够完美捕捉词语在特定语境下的歧义（例如，bank是河岸还是银行），这对于文本分类、命名实体识别（NER）和阅读理解等判别式任务至关重要 5。

##### 2.1.2 结构组件的协同作用

- 位置编码（Positional Encoding）：由于注意力机制本身具有排列不变性（Permutation Invariance），编码器必须显式注入位置信息。从早期的正弦波编码到现在的旋转位置编码（RoPE），这一组件确保了模型理解语序的能力 3。

- 残差连接与层归一化（Add & Norm）：每个子层后都接有残差连接和层归一化，这对于缓解深层网络中的梯度消失问题、确保训练稳定性起到了决定性作用 3。

#### 2.2 解码器（Decoder）：自回归生成的因果约束

解码器的架构虽然在形态上与编码器相似，但其设计哲学完全服务于自回归生成（Autoregressive Generation），即根据已生成的Token序列预测下一个Token 2。

##### 2.2.1 掩码自注意力（Masked Self-Attention）

解码器的第一个注意力层必须遵循因果律：在预测第 $t$ 个Token时，模型只能看到位置 $1$ 到 $t-1$ 的信息，绝对不能看到 $t$ 及其之后的Token。这是通过掩码（Masking）机制实现的 1。在计算Softmax之前，注意力分数矩阵的上三角部分被置为负无穷（$-\infty$），从而使这些位置的概率归零。

这种单向性（Unidirectionality）虽然限制了模型利用右侧上下文的能力，但它是实现高效生成的必要条件。在推理阶段，解码器可以利用KV缓存（KV Cache）技术，将历史Token的键和值存储起来，每次生成新Token时仅计算当前步的查询，从而避免了重复计算，极大地提高了推理效率 8。

##### 2.2.2 交叉注意力（Cross-Attention）

在标准的Encoder-Decoder架构（如Transformer、T5）中，解码器层包含第二个注意力子层——交叉注意力层 1。

- Query ($Q$)：来自解码器上一层的输出（即当前生成的文本表征）。

- Key ($K$) & Value ($V$)：来自编码器的最终输出（即源文本的完整上下文表征）。

这一机制是连接理解与生成的桥梁。例如在机器翻译中，当解码器生成中文译文时，它通过交叉注意力动态地在英文源句（由编码器处理）中检索相关信息。每一层的解码器都在反复查询编码器的输出，确保生成的内容忠实于源输入 1。

#### 2.3 架构对比总结

为了更清晰地展示两者的差异，我们从多个维度进行了对比分析：

| 维度 | 编码器 | 解码器 | 编码器-解码器 |
| --- | --- | --- | --- |
| 典型代表 | BERT, RoBERTa, DeBERTa | GPT系列, Llama, DeepSeek | Transformer (原始), T5, BART |
| 注意力流向 | 双向（全关注） | 单向（因果掩码） | 双向（Enc）+ 单向（Dec） |
| 预训练目标 | 掩码语言建模 (MLM) | 因果语言建模 (CLM/NTP) | 跨度破坏 / 去噪 |
| 优势领域 | 理解、分类、抽取 | 开放式生成、对话、代码 | 翻译、摘要、文本重写 |
| 推理特性 | 并行处理输入，非自回归 | 串行生成，依赖KV Cache | 混合模式，Enc一次计算，Dec串行 |
| 参数效率 | 相同参数下理解能力更强 | 需要更大参数量以弥补单向性 | 冗余较高，参数分散在两部分 |

---

### 3. 架构演进路线图：从BERT的崛起到Decoder-only的统治

LLM的发展史是一部关于架构选择的竞争史。尽管Encoder-Decoder架构是Transformer的鼻祖，但随后的发展出现了明显的分化，最终在2023-2025年间形成了Decoder-only架构近乎垄断的局面，尽管近期研究开始重新审视这一现状。

#### 3.1 编码器模型的黄金时代与局限 (2018-2020)

以BERT（Bidirectional Encoder Representations from Transformers）为代表的编码器模型曾统治NLP榜单。通过掩码语言建模（MLM），BERT被迫利用双向上下文来预测被遮蔽的词，这使得它在语义理解任务上具有无与伦比的优势 6。

- 应用局限：编码器模型的致命弱点在于生成能力。由于缺乏自回归生成的机制，强行用BERT进行文本生成极其低效且质量较差。这限制了其在对话系统和创意写作中的应用 5。

#### 3.2 编码器-解码器的统一尝试 (T5与BART)

为了兼顾理解与生成，Google推出了T5（Text-to-Text Transfer Transformer），将所有NLP任务统一为文本到文本的格式。无论是翻译、分类还是摘要，都通过输入文本生成输出文本 6。

- 性能权衡：T5证明了Encoder-Decoder架构在多任务学习上的强大能力。然而，研究表明，在同等参数量下，Encoder-Decoder模型的训练和推理成本通常高于Decoder-only模型，因为在生成每个Token时，解码器都需要通过交叉注意力访问编码器的状态，这增加了内存带宽的压力 10。

#### 3.3 Decoder-only架构的全面统治 (GPT-3至Llama 3)

自GPT-3展示了惊人的上下文学习（In-Context Learning, ICL）能力后，Decoder-only架构迅速成为主流。

- 缩放定律（Scaling Laws）：Decoder-only架构在参数规模扩展时表现出极其稳定的性能提升。其简单的结构（无需处理Encoder和Decoder之间的复杂交互）使得工程实现和并行训练更加容易 14。

- 上下文学习的本质：虽然从理论上讲，单向注意力限制了模型对下文的感知，但在海量数据和超大参数量的加持下，Decoder-only模型通过将Prompt作为前缀，实际上是在利用过去的Token（Prompt）来调节对未来的预测。这种机制完美契合了通用的指令-响应范式 8。

- 效率优势：在推理阶段，Decoder-only模型仅需维护一个KV Cache，且随着生成的进行线性追加。相比之下，Encoder-Decoder模型在多轮对话中可能需要反复重新编码历史上下文，或者维护更复杂的缓存机制 8。

#### 3.4 反思与挑战：RedLLM与架构复兴

尽管Decoder-only占据主导，但2025年的研究指出这种单一化可能并非最优解。一项名为RedLLM的研究对比了在相同数据（RedPajama V1 1.6T tokens）下训练的Encoder-Decoder模型与Decoder-only模型。

- 结论惊人：RedLLM（Encoder-Decoder）在指令微调后，展现出与Decoder-only模型相当甚至更好的零样本和少样本性能，且在某些任务上推理效率更高 17。

- 双向注意力的价值：实验表明，RedLLM受益于编码器的双向注意力机制，在处理需要深度理解输入的任务时更具优势。这暗示了在追求极致生成能力的同时，我们可能低估了编码器架构在特定场景下的潜力 17。

---

### 4. 效率革命：混合专家（MoE）与注意力机制的重构

随着模型参数迈向万亿级别，稠密（Dense）模型的训练和推理成本变得不可持续。2024-2025年，以DeepSeek-V3、Mixtral和Gemini 1.5为代表的模型，通过引入混合专家（MoE）架构和新型注意力机制，成功打破了参数量-计算量的线性约束。

#### 4.1 混合专家（MoE）架构原理

MoE的核心思想是将前馈网络（FFN）层替换为多个并行的专家（Expert）网络，并通过一个路由（Router/Gate）网络动态决定每个Token由哪些专家处理 18。

##### 4.1.1 稀疏激活（Sparse Activation）

在DeepSeek-V3（671B参数）中，对于每一个Token，系统并不会激活所有参数，而仅激活约37B参数（包括共享专家和选中的路由专家）。这意味着模型拥有671B的知识容量，但推理时的计算量仅相当于一个37B的模型 20。

- 优势：这种稀疏性使得模型能够在保持极高智能水平的同时，大幅降低推理延迟和能源消耗。

- 挑战：传统MoE面临专家负载不均衡的问题（即某些专家过劳，某些专家闲置），这通常需要引入辅助损失（Auxiliary Loss）来强制平衡，但这可能会损害模型性能 22。

##### 4.1.2 DeepSeekMoE的创新路由机制

DeepSeek-V3引入了一种独特的DeepSeekMoE架构，其路由策略与传统的Top-K路由有显著不同：

1. 共享专家与路由专家分离（Shared + Routed Experts）：DeepSeek将一部分专家设定为共享专家，这些专家始终被激活，负责捕获所有Token通用的基础知识（如语法结构）。另一部分为路由专家，仅针对特定语义的Token激活 23。

2. 细粒度专家分割（Fine-Grained Segmentation）：相比于Switch Transformer等早期MoE，DeepSeek将专家切分得更细（例如将一个大FFN切分为多个小FFN），使得知识的组合更加灵活 23。

3. 无辅助损失负载均衡（Auxiliary-Loss-Free Load Balancing）：DeepSeek-V3摒弃了传统的辅助损失，转而通过调整路由偏置项（Bias）来实现负载均衡。这种方法避免了辅助损失对主训练目标的干扰，实现了极其稳定的训练过程（全程无Loss Spike）20。

#### 4.2 多头潜在注意力（MLA）：打破KV缓存瓶颈

在长上下文推理中，键值缓存（KV Cache）占用的显存往往超过模型权重本身。对于一个671B的模型，标准的FP16 KV Cache可能高达数TB，这使得单卡或少卡推理成为不可能。

DeepSeek-V3提出的多头潜在注意力（Multi-Head Latent Attention, MLA）是解决这一痛点的关键技术 20。

- 原理：MLA不再直接存储巨大的键（K）和值（V）矩阵，而是通过低秩矩阵分解技术，将KV投影到一个低维的潜在向量（Latent Vector）中进行存储。在计算注意力时，再临时恢复出所需的头特定信息 24。

- 数据对比：

    - 标准MHA：对于类似规模的模型，KV Cache可能高达 **213.5 GB**。

    - MLA优化后：KV Cache仅需 **7.6 GB**，压缩比达到惊人的 **93%（约28倍）** 28。

- 意义：这意味着即使是超大规模的DeepSeek-V3，也能在较少的GPU资源上支持极长的上下文窗口，且推理吞吐量大幅提升，同时在模型性能上优于分组查询注意力（GQA）29。

#### 4.3 Google Gemini 1.5 Pro：百万级上下文的MoE实践

Google的Gemini 1.5 Pro是另一个MoE架构的巅峰之作，其最大的亮点是支持高达 **1000万Token** 的上下文窗口 31。

- 长程检索：借助MoE架构，Gemini 1.5 Pro能够在其巨大的记忆中精准定位到微小的细节（如在数小时的视频或数百万字的代码库中找到特定片段）。这得益于MoE将不同类型的上下文处理分配给不同的专家，避免了单一网络处理超长序列时的遗忘或混淆问题 32。

---

### 5. 超越注意力机制：线性模型与SSM的复兴

尽管Transformer即MoE优化极其成功，但其自注意力机制的核心计算复杂度仍为 $O(N^2)$（随序列长度平方增长）。为了从根本上解决长序列处理的效率问题，2024-2025年见证了状态空间模型（State Space Models, SSM）和线性注意力（Linear Attention）的爆发式增长。

#### 5.1 Mamba与选择性SSM架构

Mamba架构的提出是对Transformer霸权的最有力挑战之一。它基于结构化状态空间模型（S4），但引入了关键的选择性机制（Selection Mechanism）34。

- 线性缩放：Mamba的推理复杂度为 $O(N)$（线性）。它通过一个固定大小的隐状态（Hidden State）来递归地处理序列，类似于RNN，但通过硬件感知算法（Hardware-aware Parallel Algorithm）实现了并行训练 36。

- 输入依赖性：传统SSM的转换矩阵 $A, B, C$ 是静态的，而Mamba使 $B, C$ 矩阵和步长 $\Delta$ 成为输入 $x$ 的函数。这意味着模型可以根据当前输入动态决定记住什么信息、遗忘什么信息。这解决了传统RNN无法根据上下文动态调整记忆内容的缺陷 34。

- 性能对比：在语言建模、基因组学和音频处理等任务上，Mamba-3B在同等参数下性能超越了Transformer，且在推理吞吐量上高出5倍，随着序列长度增加，优势呈指数级扩大 36。

#### 5.2 混合架构的崛起：Jamba与Granite 4.0

纯SSM模型在处理某些需要精准复制粘贴或高度依赖历史细节检索的任务时，仍不如全注意力机制的Transformer 38。因此，工业界开始转向混合架构（Hybrid Architecture）。

- IBM Granite 4.0：采用了Mamba-Transformer混合架构，具体比例为 **9:1**（即9层Mamba层搭配1层Transformer层）。这种设计利用Mamba层高效处理全局上下文压缩，利用稀疏的Transformer层处理局部高精度的注意力机制 39。

    - 效益：相比传统Transformer，Granite 4.0在处理长序列时内存占用减少了 **70%**，且在消费级GPU上也能实现极高的推理速度 39。

- Gated DeltaNet：这是一种新型的线性Transformer变体，结合了门控机制（Gating）和Delta更新规则。门控允许模型快速擦除无关记忆，而Delta规则允许对记忆进行精确修改。基准测试显示，Gated DeltaNet在语言建模和零样本常识推理任务上持续优于Mamba2，证明了线性模型在复杂任务上的潜力 40。

#### 5.3 线性注意力与Softmax注意力的理论博弈

线性注意力试图通过核技巧（Kernel Trick）将 $O(N^2)$ 的Softmax注意力近似为线性复杂度。

- 理论差异：传统Softmax注意力的优势在于其强大的局部建模能力和尖峰分布（Peaky Distribution），能够极度聚焦于某个关键Token。线性注意力由于去除了Softmax的非线性缩放，往往导致注意力分布过于平滑，难以捕捉高频细节 42。

- 最新进展：研究表明，通过特定的映射函数（如ReLU或基于Mamba的门控）恢复注意力的聚焦能力，线性注意力可以在保持 $O(N)$ 效率的同时，逼近Softmax注意力的性能 42。

---

### 6. 前沿探索：动态记忆与测试时训练（Test-Time Training）

进入2026年，LLM的研究前沿已经从静态架构转向了动态适应架构。核心理念是：模型不应在训练结束后就停止学习，而应在推理阶段（Test-Time）继续根据上下文更新自身的参数。

#### 6.1 测试时训练（TTT）：将上下文视为训练数据

传统的Transformer通过KV Cache存储上下文，这本质上是一种显式的记忆。而Test-Time Training (TTT) 提出了一种革命性的观点：上下文就是训练数据 44。

- 机制原理（TTT-E2E）：在处理输入序列时，TTT模型并不仅仅是前向传播，而是利用当前的上下文通过自监督目标（如预测下一个Token）进行实时的梯度下降，更新模型内部的隐藏状态（Hidden States）或特定权重。这相当于将上下文信息压缩（Compress）进了模型的权重中，而不是存储在缓存里 44。

- 双层优化循环：TTT-E2E包含两个循环。内层循环在推理时运行，更新权重以适应当前上下文；外层循环在元训练（Meta-training）阶段运行，优化模型的初始权重，使其能够通过内层循环快速学习 44。

- 性能突破：

    - 无限上下文：由于信息被压缩进权重，TTT模型的推理延迟是常数 $O(1)$，与上下文长度无关。

    - 速度对比：在128k长度下，TTT-E2E比全注意力Transformer快 **2.7倍**；在2M长度下，快 **35倍** 44。

    - 效果：这种方法解决了RNN类模型在长序列中记忆衰退的问题，同时避免了Transformer的计算墙。

#### 6.2 Titans与神经记忆模块

Google提出的Titans架构采取了另一条路径：引入独立的神经记忆模块（Neural Memory Module）47。

- 记忆与遗忘：Titans利用一个类似于神经硬盘的结构来存储历史信息。与TTT不同，Titans设计了显式的遗忘机制（Forgetting Mechanism）。当遇到惊奇度（Surprise）较低的信息（即模型已经预测准确的信息）时，模型会选择性忽略，仅将高惊奇度的信息写入长期记忆。这使得Titans能够在有限的内存容量下处理理论上无限的上下文流 47。

---

### 7. 性能基准与经济学分析

架构的选择最终是性能与成本的博弈。以下数据展示了不同架构在实际应用中的经济学特征。

#### 7.1 训练成本分析

DeepSeek-V3的发布展示了架构优化带来的巨大成本红利。

- DeepSeek-V3 (MoE): 训练671B参数模型（14.8T tokens）仅消耗 **278.8万 H800 GPU小时**。相比之下，如果使用同等规模的稠密模型，计算成本将高出数倍 20。

- MoE的通信开销：尽管计算量减少，MoE在跨节点训练时面临巨大的All-to-All通信压力。DeepSeek通过FP8混合精度训练和优化的通信重叠技术，几乎完全掩盖了通信延迟 20。

#### 7.2 推理吞吐量对比

| 架构类型 | 模型案例 | 上下文长度 | 推理延迟特性 | 适用场景 |
| --- | --- | --- | --- | --- |
| Dense Transformer | Llama 3-70B | 短/中 | $O(N)$ (KV Cache导致) | 通用任务，高精度要求 |
| Sparse MoE | DeepSeek-V3 | 长 (128k) | 优于Dense，受限于激活参数 | 高并发、复杂推理、长文档 |
| Linear SSM | Mamba-2 | 超长 (>1M) | $O(1)$ (常数级) | 基因测序、超长文本分析 |
| TTT-E2E | TTT-3B | 极长 (2M+) | $O(1)$ (常数级，且比SSM更准) | 需要实时学习的流式任务 |

- 硬件适配：在消费级硬件（如Apple M2 Ultra）上，经过4-bit量化的Llama-7B（Decoder-only）可达到约 **20-30 tokens/s** 的速度。而Encoder-Decoder模型（如T5）由于解码时需反复关注编码器输出，在相同量化下推理速度通常较慢 49。

---

### 8. 未来展望：2026年及以后的架构趋势

基于当前的技术爆发，我们可以预见2026年LLM架构的几个关键演变方向：

#### 8.1 递归语言模型（Recursive Language Models）与上下文折叠

未来的模型将不再被动地接收超长上下文，而是具备主动管理上下文的能力。这被称为上下文折叠（Context Folding）。模型会主动将之前的交互历史压缩成高密度的向量或摘要，或者将其卸载到外部存储中，仅在需要时检索。这与TTT的理念不谋而合，即记忆应内化为模型的一部分 50。

#### 8.2 代理式架构（Agentic Architectures）的标准化

单一模型包打天下的时代正在结束。未来的系统将是多模型协作的复合体：

- System 1（快思考）：由Mamba或线性模型构成，负责处理海量的实时数据流，拥有极高的吞吐量。

- System 2（慢思考）：由MoE或全注意力Transformer构成，仅在System 1检测到高难度任务时激活，进行深度的逻辑推理 51。

#### 8.3 混合架构的常态化

纯Transformer架构将逐渐边缘化。Mamba for Context, Attention for Reasoning（用Mamba处理上下文，用Attention处理推理）将成为标准配置。IBM Granite 4.0和Jamba已经证明了这种混合架构在企业级应用中的巨大潜力 39。

---

### 9. 结论

从最初的Transformer编码器-解码器架构，到Decoder-only的单极霸权，再到如今MoE、MLA、SSM和TTT带来的百花齐放，LLM的架构演进始终围绕着两个核心矛盾：表达能力与计算效率的平衡，以及记忆与计算的转化。

当前（2025-2026年初）的现状表明：

1. Decoder-only + MoE 是当前通用大模型的最佳实践（如DeepSeek-V3、GPT-4），通过稀疏激活和MLA技术解决了参数规模和显存墙的问题。

2. Encoder-Decoder 并未消亡，它在RedLLM等研究中证明了其在特定任务上的高效性，且在多模态理解（Video-to-Text）中仍扮演关键角色。

3. 非Transformer架构（Mamba、TTT）正在长上下文和端侧设备领域通过线性复杂度和动态权重更新机制，开辟出全新的赛道。

随着计算硬件的物理极限日益逼近，未来的突破将更多来自于类似DeepSeekMoE和TTT这样的算法层面的底层创新，而非单纯的算力堆叠。

---

**<font color="#2ecc71">✅ 已格式化</font>**
